{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>\n",
    "Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 环境配置   本代码主要参考官方baseline\n",
    "\n",
    "目前飞桨（PaddlePaddle）正式版仍为1.8.4，以下代码均为2.0RC0测试版本，本地安装需要指定版本号进安装。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 数据预处理 - 配置部分 （代码并没有保存CheckPoint）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from paddle.io import Dataset\n",
    "from baseline_tools import *\n",
    "\n",
    "DATA_RATIO = 0.9  # 训练集和验证集比例\n",
    "\n",
    "# None表示不使用，“emb”为Embedding预处理方案，选手可自由选择使用字段以及预处理方案\n",
    "TAGS = {'android_id': None,\n",
    "        'apptype': \"emb\",\n",
    "        'carrier': \"emb\",\n",
    "        'dev_height': 'emb',\n",
    "        'dev_ppi': 'emb',\n",
    "        'dev_width': 'emb',\n",
    "        'lan': 'emb',\n",
    "        'media_id': \"emb\",\n",
    "        'ntt': \"emb\",\n",
    "        'os': \"emb\",\n",
    "        'osv': 'emb',\n",
    "        'package': \"emb\",\n",
    "        'sid': None,\n",
    "        'timestamp': \"norm\",\n",
    "        'version': \"emb\",\n",
    "        'fea_hash': 'norm',\n",
    "        'location': \"emb\",\n",
    "        'fea1_hash': 'norm',\n",
    "        'cus_type': 'emb'}\n",
    "\n",
    "# 归一化权重设置\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 数据预处理 - 生成Embedding所需数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete\n"
     ]
    }
   ],
   "source": [
    "datas = pd.read_csv(\"train.csv\")\r\n",
    "for ids,data in enumerate(datas[\"fea_hash\"]):\r\n",
    "    try:\r\n",
    "        data = float(data)\r\n",
    "    except:\r\n",
    "        datas[\"fea_hash\"][ids] = 499997879\r\n",
    "        print(ids+1)\r\n",
    "datas.to_csv(\"train.csv\")\r\n",
    "#fea_hash字段中有许多异常数据，将他们改为最大值的一半：499997879.0\r\n",
    "datas = pd.read_csv(\"test.csv\",dtype=str)\r\n",
    "#datas = datas[\"fea_hash\"]\r\n",
    "#print(datas.head)\r\n",
    "\r\n",
    "for ids,data in enumerate(datas[\"fea_hash\"]):\r\n",
    "    try:\r\n",
    "        data = float(data)\r\n",
    "    except:\r\n",
    "        datas[\"fea_hash\"][ids] = 499997879\r\n",
    "        print(ids+1)\r\n",
    "datas = datas\r\n",
    "datas.to_csv(\"test.csv\")\r\n",
    "print('complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apptype 字典生成完毕,共 89 个id\n",
      "carrier 字典生成完毕,共 5 个id\n",
      "dev_height 字典生成完毕,共 798 个id\n",
      "dev_ppi 字典生成完毕,共 92 个id\n",
      "dev_width 字典生成完毕,共 346 个id\n",
      "lan 字典生成完毕,共 22 个id\n",
      "media_id 字典生成完毕,共 284 个id\n",
      "ntt 字典生成完毕,共 8 个id\n",
      "os 字典生成完毕,共 2 个id\n",
      "osv 字典生成完毕,共 155 个id\n",
      "package 字典生成完毕,共 1950 个id\n",
      "timestamp字段的最小值为1559491201174.7812，normal字段1.6534305617481573e-09\n",
      "version 字典生成完毕,共 22 个id\n",
      "fea_hash字段的最小值为0.0，normal字段2.3283201561138293e-10\n",
      "location 字典生成完毕,共 332 个id\n",
      "fea1_hash字段的最小值为12400.0，normal字段2.329966199378393e-10\n",
      "cus_type 字典生成完毕,共 58 个id\n",
      "全部生成完毕\n"
     ]
    }
   ],
   "source": [
    "TRAIN_PATH = \"train.csv\"\n",
    "SAVE_PATH = \"emb_dicts\"\n",
    "df = pd.read_csv(TRAIN_PATH, index_col=0)\n",
    "pack = dict()\n",
    "for tag, tag_method in TAGS.items():\n",
    "    if tag_method != \"emb\":\n",
    "        if tag_method == 'norm':\n",
    "            print('{}字段的最小值为{}，normal字段{}'.format(tag,float(min(df.loc[:,tag])),1/(float(max(df.loc[:,tag]))-float(min(df.loc[:,tag])))))\n",
    "        continue\n",
    "    else:\n",
    "        data = df.loc[:, tag]\n",
    "        dict_size = make_dict_file(data, SAVE_PATH, dict_name=tag)\n",
    "        pack[tag] = dict_size + 1  # +1是为了增加字典中不存在的情况，提供一个默认值\n",
    "\n",
    "with open(os.path.join(SAVE_PATH, \"size.dict\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(str(pack))\n",
    "\n",
    "print(\"全部生成完毕\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 数据预处理 - 定义数据读取器以及预处理流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_size_dict(dict_path=\"./emb_dicts/size.dict\"):\n",
    "    \"\"\"\n",
    "    获取Embedding推荐大小\n",
    "    :param dict_path: 由run_make_emb_dict.py生成的size.dict\n",
    "    :return: 推荐大小字典{key: num}\n",
    "    \"\"\"\n",
    "    with open(dict_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        try:\n",
    "            size_dict = eval(f.read())\n",
    "        except Exception as e:\n",
    "            print(\"size_dict打开失败，请检查\", dict_path, \"文件是否正常，报错信息如下:\\n\", e)\n",
    "        return size_dict\n",
    "class Data2IdNorm:\n",
    "    \"\"\"\n",
    "    数据归一化类\n",
    "    \"\"\"\n",
    "    def __init__(self, norm_weight,norm_min):\n",
    "        self.norm_weight = norm_weight\n",
    "        self.norm_min = norm_min\n",
    "    def transform_data(self, sample, shape=None, d_type=\"float32\"):\n",
    "        sample = (float(sample)-self.norm_min)*self.norm_weight\n",
    "        sample = value2numpy(sample, shape, d_type)\n",
    "        return sample\n",
    "\n",
    "    def get_method(self):\n",
    "        return self.transform_data\n",
    "\n",
    "class Reader(Dataset):\n",
    "    def __init__(self,\n",
    "                 is_infer: bool = False,\n",
    "                 is_val: bool = False,\n",
    "                 use_mini_train: bool = False,\n",
    "                 emb_dict_path=\"./emb_dicts\"):\n",
    "\n",
    "        \"\"\"\n",
    "        数据读取类\n",
    "        :param is_infer: 是否为预测Reader\n",
    "        :param is_val: 是否为验证Reader\n",
    "        :param use_mini_train：使用Mini数据集\n",
    "        :param emb_dict_path: emb字典路径\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # 选择文件名\n",
    "        train_name = \"mini_train\" if use_mini_train else \"train\"\n",
    "        file_name = \"test\" if is_infer else train_name\n",
    "        # 根据文件名读取对应csv文件\n",
    "        df = pd.read_csv(file_name + \".csv\")\n",
    "        # 划分数据集\n",
    "        if is_infer:\n",
    "            self.df = df.reset_index(drop=True)\n",
    "        else:\n",
    "            start_index = 0 if not is_val else int(len(df) * DATA_RATIO)\n",
    "            end_index = int(len(df) * DATA_RATIO) if not is_val else len(df)\n",
    "            self.df = df.loc[start_index:end_index].reset_index(drop=True)\n",
    "        # 数据预处理\n",
    "        NORM_WEIGHT = {'timestamp':1.6534305617481573e-09,'fea_hash':2.3283201561138293e-10,'fea1_hash':2.3299594677571534e-10}\n",
    "        zuixiaozhi = {'timestamp':1559491201174.7812,'fea_hash':0.0,'fea1_hash':12400.0}\n",
    "        self.cols = [tag for tag, tag_method in TAGS.items() if tag_method is not None]\n",
    "        self.methods = dict()\n",
    "        for col in self.cols:\n",
    "            # ===== 预处理方法注册 =====\n",
    "\n",
    "            if TAGS[col] == \"emb\":\n",
    "                self.methods[col] = Data2IdEmb(dict_path=emb_dict_path, dict_name=col).get_method()\n",
    "            elif TAGS[col] == \"norm\":\n",
    "                self.methods[col] = Data2IdNorm(norm_weight=NORM_WEIGHT[col],norm_min=zuixiaozhi[col]).get_method()\n",
    "\n",
    "        # 设置FLAG负责控制__getitem__的pack是否包含label\n",
    "        self.add_label = not is_infer\n",
    "        # 设置FLAG负责控制数据集划分情况\n",
    "        self.is_val = is_val\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        获取sample\n",
    "        :param index: sample_id\n",
    "        :return: sample\n",
    "        \"\"\"\n",
    "        # 因为本次数据集的字段非常多，这里就使用一个列表来\"收纳\"这些数据\n",
    "        pack = []\n",
    "        # 遍历指定数量的字段\n",
    "        for col in self.cols:\n",
    "            sample = self.df.loc[index, col]\n",
    "            sample = self.methods[col](sample)\n",
    "            pack.append(sample)\n",
    "\n",
    "        # 如果不是预测，则添加标签数据\n",
    "        if self.add_label:\n",
    "            tag_data = self.df.loc[index, \"label\"]\n",
    "            tag_data = np.array(tag_data).astype(\"int64\")\n",
    "            pack.append(tag_data)\n",
    "            return pack\n",
    "        else:\n",
    "            return pack\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 数据预处理 - 检查数据是否可以正常读取（可选）\n",
    "默认只检查训练，infer和test可以在`val_reader = Reader(此处设置)`中参考刚刚定义的`Reader`来配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检查数据ing...\n"
     ]
    }
   ],
   "source": [
    "# 用于独立测试数据读取是否正常 推荐在本地IDE中下断点进行测试\n",
    "print(\"检查数据ing...\")\n",
    "val_reader = Reader()\n",
    "\n",
    "print(len(val_reader))\n",
    "for data_id, data in enumerate(val_reader):\n",
    "  \n",
    "    for i in range(len(data)):\n",
    "        if data_id == 0:\n",
    "            print(\"第\", i, \"个字段 值为：\", data[i])\n",
    "        else:\n",
    "            break\n",
    "    if data_id % 1000 == 0:\n",
    "        print(\"第\", data_id, \"条数据可正常读取 正在检查中\", end=\"\\r\")\n",
    "    if data_id == len(val_reader) - 1: \n",
    "        print(\"数据检查完毕\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(val_reader.methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**以上对于数据的预处理参考了官方的数据处理方法，自己尝试了几种其他的组合方案但是效果并不好。所以最后还是采用了官方的baseline方案，这里应该还有不少的优化空间。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 训练&推理配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import paddle\r\n",
    "import paddle.nn as nn\r\n",
    "import paddle.tensor as tensor\r\n",
    "from paddle.static import InputSpec\r\n",
    "from paddle.metric import Accuracy\r\n",
    "\r\n",
    "# 模型保存与加载文件夹\r\n",
    "SAVE_DIR = \"./output/\"\r\n",
    "\r\n",
    "# 部分训练超参数\r\n",
    "EPOCHS = 30  # 训练多少个循环\r\n",
    "TRAIN_BATCH_SIZE = 128  # mini_batch 大小\r\n",
    "EMB_SIZE = 64  # Embedding特征大小\r\n",
    "EMB_LINEAR_SIZE =  128 # Embedding后接Linear层神经元数量\r\n",
    "LINEAR_LAYERS_NUM = 8  # 归一化方案的Linear层数量\r\n",
    "class line(nn.Layer):\r\n",
    "    def __init__(self,dim,hidden_dim):\r\n",
    "        super().__init__()\r\n",
    "        self.lay = nn.Sequential(\r\n",
    "            nn.LayerNorm(dim),\r\n",
    "            nn.Linear(dim,hidden_dim),\r\n",
    "            nn.GELU(),\r\n",
    "            nn.Dropout(0.2),\r\n",
    "            nn.Linear(hidden_dim,dim)\r\n",
    "        )\r\n",
    "    def forward(self,x):\r\n",
    "        return self.lay(x)\r\n",
    "class mat(nn.Layer):\r\n",
    "    def __init__(self,dim,heads):\r\n",
    "        super().__init__()\r\n",
    "        dim_head = dim//heads\r\n",
    "        self.heads = heads\r\n",
    "        inner_dim = dim * heads\r\n",
    "        self.dh = dim_head**-0.5\r\n",
    "        self.to_output = nn.Sequential(\r\n",
    "            nn.Linear(inner_dim,dim),\r\n",
    "            nn.Dropout(0.2)\r\n",
    "        )\r\n",
    "        self.to_kqv = nn.Linear(dim,inner_dim*3,bias_attr=None)\r\n",
    "    def forward(self,x):\r\n",
    "        b,n,_ = x.shape\r\n",
    "        qkv = self.to_kqv(x).chunk(3,axis=-1)\r\n",
    "        q,k,v = map(lambda t: paddle.reshape(t,[b,self.heads,n,-1]),qkv)\r\n",
    "        dots = paddle.matmul(q,k,transpose_y=True)*self.dh\r\n",
    "        soft = nn.functional.softmax(dots,axis= -1)\r\n",
    "        out =paddle.matmul(soft,v)\r\n",
    "        out = paddle.reshape(out,[b,n,-1])\r\n",
    "        out = self.to_output(out)\r\n",
    "        return out\r\n",
    "class PreNorm(nn.Layer):\r\n",
    "    def __init__(self,dim,fn):\r\n",
    "        super().__init__()\r\n",
    "        self.norm = nn.LayerNorm(dim)\r\n",
    "        self.fn = fn\r\n",
    "    def forward(self,x):\r\n",
    "        return self.fn(self.norm(x))\r\n",
    "class transformer(nn.Layer):\r\n",
    "    def __init__(self,dim,number):\r\n",
    "        super().__init__()\r\n",
    "        self.lay = nn.LayerList([])\r\n",
    "        for _ in range(number):\r\n",
    "            self.lay.append(\r\n",
    "            nn.LayerList([cancha(PreNorm(dim,mat(dim,8))),\r\n",
    "            cancha(PreNorm(dim,line(dim,int(dim/2))))\r\n",
    "            ]))\r\n",
    "            \r\n",
    "    def forward(self,x):\r\n",
    "        for lay1,lay2 in self.lay:\r\n",
    "            x = lay1(x)\r\n",
    "            x = lay2(x)\r\n",
    "        return x\r\n",
    "# 配置训练环境\r\n",
    "USE_MINI_DATA = False  # 默认使用小数据集，此方法可加快模型产出速度，但可能会影响准确率\r\n",
    "class cancha(nn.Layer):\r\n",
    "    def __init__(self,fc):\r\n",
    "        super().__init__()\r\n",
    "        self.fc = fc\r\n",
    "  \r\n",
    "    def forward(self,x):\r\n",
    "        y = self.fc(x)\r\n",
    "\r\n",
    "        y += x\r\n",
    "        return y\r\n",
    "# 组网\r\n",
    "class SampleNet(paddle.nn.Layer):\r\n",
    "    def __init__(self, tag_dict: dict, size_dict: dict):\r\n",
    "        # 继承Model\r\n",
    "        super().__init__()\r\n",
    "        # 新建一个隐藏层列表，用于存储各字段隐藏层对象\r\n",
    "      \r\n",
    "        # 定义一个用于记录输出层的输入大小变量，经过一个emb的网络结构就增加该结构的output_dim，以此类推\r\n",
    "        out_layer_input_size = 0\r\n",
    "        # 遍历每个字段以及其处理方式\r\n",
    "        self.hidden_layers_list = nn.LayerList([])\r\n",
    "        for tag, tag_method in tag_dict.items():\r\n",
    "            # ===== 网络结构方法注册 =====\r\n",
    "            # Embedding方法注册\r\n",
    "            if tag_method == \"emb\":\r\n",
    "                emb = nn.Embedding(num_embeddings=size_dict[tag], embedding_dim=EMB_SIZE)#定义多个字段的emb处理方法，添加到一个列表中\r\n",
    "                self.hidden_layers_list.append(emb)\r\n",
    "            elif tag_method == \"norm\":\r\n",
    "                continue\r\n",
    "            elif tag_method is None:\r\n",
    "                continue\r\n",
    "        self.dict_list = ['emb','emb','emb','emb','emb', 'emb', 'emb', 'emb', 'emb','emb', 'norm', 'emb', 'norm', 'emb', 'norm', 'emb']\r\n",
    "     \r\n",
    "        self.con1= nn.Conv1D(in_channels=1,out_channels=32,kernel_size=3,stride=1,padding = 1)\r\n",
    "\r\n",
    "        \r\n",
    "        #self.hidden_layer = nn.LSTM(EMB_SIZE, 128, 1)\r\n",
    "        #self.hidden_layer = mat(EMB_SIZE,8)\r\n",
    "        self.hidden_layer = transformer(EMB_SIZE,5)\r\n",
    "        '''\r\n",
    "        self.hidden_layer1 = nn.Sequential(\r\n",
    "                                        nn.Conv1D(in_channels=1,out_channels=32,kernel_size=8,stride=8),\r\n",
    "                                        nn.ReLU(),\r\n",
    "                                        nn.Conv1D(in_channels=32,out_channels=16,kernel_size=3,stride=1,padding=1),\r\n",
    "                                        nn.ReLU(),\r\n",
    "                                        nn.Conv1D(in_channels=16,out_channels=1,kernel_size=3,stride=1,padding=1)\r\n",
    "                                            \r\n",
    "                                        )\r\n",
    "        '''\r\n",
    "        self.con2 = nn.Conv1D(in_channels=32,out_channels=1,kernel_size=3,stride=1,padding = 1)\r\n",
    "        self.hidden_layer2=nn.Sequential(  \r\n",
    "            nn.Linear(in_features=64, out_features=32),\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.Linear(in_features=32,out_features=16),\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.Dropout(0.2),\r\n",
    "            nn.Linear(in_features=16,out_features=8)\r\n",
    "                                                )\r\n",
    "        \r\n",
    "        \r\n",
    "        # 归一化方法注册\r\n",
    "    \r\n",
    "        self.hidden_layer_norm1 =  nn.LSTM(1,32)\r\n",
    "        self.hidden_layer_norm2 = transformer(32,5)\r\n",
    "        self.hidden_layer_norm=nn.Sequential(\r\n",
    "            nn.LayerNorm(32),\r\n",
    "            nn.Linear(in_features= 32,out_features = 16),\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.Dropout(0.2),\r\n",
    "            nn.Linear(in_features = 16,out_features = 1)\r\n",
    "        )\r\n",
    "        # 定义输出层，因为是二分类任务，激活函数以及损失方案可以由选手自己发挥，此处为sigmoid激活函数\r\n",
    "        # Tips: 若使用sigmoid激活函数，需要修改output_dim和损失函数，推荐理解原理后再尝试修改\r\n",
    "        self.out_layers = nn.Sequential(\r\n",
    "            nn.LayerNorm(107),\r\n",
    "            nn.Linear(in_features=107,out_features=64),\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.Dropout(0.2),\r\n",
    "            nn.Linear(in_features=64,out_features=2)\r\n",
    "            )\r\n",
    "\r\n",
    "    # 前向推理部分 `*input_data`的`*`表示传入任一数量的变量\r\n",
    "    def forward(self, *input_data):\r\n",
    "        layer_list = []  # 用于存储各字段特征结果\r\n",
    "        number = 0\r\n",
    "        for sample_data ,tag_method in zip(input_data,self.dict_list):\r\n",
    "            \r\n",
    "            if tag_method == 'norm':\r\n",
    "                tmp = sample_data\r\n",
    "                tmp = tmp.astype('float32')\r\n",
    "                tmp = tmp.unsqueeze(1)\r\n",
    "                tmp,(l,m) = self.hidden_layer_norm1(tmp)\r\n",
    "                tmp = self.hidden_layer_norm2(tmp)\r\n",
    "                tmp = self.hidden_layer_norm(tmp)\r\n",
    "            else:\r\n",
    "                emb = self.hidden_layers_list[number]\r\n",
    "                tmp = emb(sample_data.astype('int64'))\r\n",
    "                tmp = tmp.astype('float32')\r\n",
    "                #tmp = self.con1(tmp)\r\n",
    "                #tmp = self.hidden_layer(tmp)\r\n",
    "                #tmp = self.con2(tmp)\r\n",
    "                #tmp = self.hidden_layer1(tmp)\r\n",
    "                tmp = self.hidden_layer(tmp)\r\n",
    "                tmp = self.hidden_layer2(tmp)\r\n",
    "                number += 1\r\n",
    "            layer_list.append(tensor.flatten(tmp, start_axis=1))  # flatten是因为原始shape为[batch size, 1 , *n], 需要变换为[bs, n]\r\n",
    "        # 对所有字段的特征合并\r\n",
    "        \r\n",
    "        layers = tensor.concat(layer_list, axis=1)\r\n",
    "        # 把特征放入用于输出层的网络\r\n",
    "        result = self.out_layers(layers)\r\n",
    "        #result = paddle.nn.functional.softmax(result)\r\n",
    "        # 返回分类结果\r\n",
    "        return result\r\n",
    "# 定义网络输入\r\n",
    "inputs = []\r\n",
    "for tag_name, tag_m in TAGS.items():\r\n",
    "    d_type = \"float32\"\r\n",
    "    if tag_m == \"emb\":\r\n",
    "        d_type = \"int64\"\r\n",
    "    if tag_m is None:\r\n",
    "        continue\r\n",
    "    inputs.append(InputSpec(shape=[-1, 1], dtype=d_type, name=tag_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**这里采用了最近很火的transformer框架，并且加上了LSTM进行优化，本来还加上了卷积部分，但是加上卷积之后训练时间变得过长，并且准确率并没有什么提升所以还是放弃了卷积结构**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 执行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "use_gpu = True\r\n",
    "paddle.set_device('gpu:0') if use_gpu else paddle.set_device('cpu')\r\n",
    "place = paddle.CUDAPlace(0)\r\n",
    "learn = paddle.optimizer.lr.StepDecay(learning_rate=0.007,step_size=3516,gamma=0.8,verbose = False)\r\n",
    "# 定义Label\r\n",
    "labels = [InputSpec([-1, 1], 'int64', name='label')]\r\n",
    "# 实例化SampleNet\r\n",
    "model = paddle.Model(SampleNet(TAGS, get_size_dict()), inputs=inputs, labels=labels)\r\n",
    "# 获取训练集和测试集数据读取器\r\n",
    "train_reader = Reader(use_mini_train=USE_MINI_DATA)\r\n",
    "val_reader = Reader(use_mini_train=USE_MINI_DATA, is_val=True)\r\n",
    "# 定义优化器\r\n",
    "optimizer = paddle.optimizer.Adam(learning_rate=learn, parameters=model.parameters())\r\n",
    "# 模型训练配置\r\n",
    "model.prepare(optimizer, paddle.nn.loss.CrossEntropyLoss(), Accuracy())\r\n",
    "# 开始训练\r\n",
    "model.fit(train_data=train_reader,  # 训练集数据\r\n",
    "            eval_data=val_reader,  # 交叉验证集数据\r\n",
    "            batch_size=TRAIN_BATCH_SIZE,  # Batch size大小\r\n",
    "            epochs=EPOCHS,  # 训练轮数\r\n",
    "            log_freq=500,  # 日志打印间隔\r\n",
    "            save_dir=SAVE_DIR)  # checkpoint保存路径\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 执行推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 推理部分\r\n",
    "CHECK_POINT_ID = \"1\"  # 如没有训练完毕，可以挑选SAVE_DIR路径的下的中的checkpoint文件名(不包含拓展名哦)，例如\"1\"\r\n",
    "TEST_BATCH_SIZE = 128  # 若因内存/显存发生报错，请优先调整为1\r\n",
    "RESULT_FILE = \"./result1.csv\"  # 推理文件保存位置\r\n",
    "\r\n",
    "# 实例化SampleNet\r\n",
    "model = paddle.Model(SampleNet(TAGS, get_size_dict()), inputs=inputs)\r\n",
    "# 获取推理Reader并读取参数进行推理\r\n",
    "infer_reader = Reader(is_infer=True)\r\n",
    "model.load(os.path.join(SAVE_DIR, CHECK_POINT_ID))\r\n",
    "# 开始推理\r\n",
    "model.prepare()\r\n",
    "infer_output = model.predict(infer_reader, TEST_BATCH_SIZE)\r\n",
    "# 获取原始表中的字段并添加推理结果\r\n",
    "result_df = infer_reader.df.loc[:, \"sid\"]\r\n",
    "pack = []\r\n",
    "for batch_out in infer_output[0]:\r\n",
    "    for sample in batch_out:\r\n",
    "        pack.append(np.argmax(sample))\r\n",
    "# 保存csv文件\r\n",
    "result_df = pd.DataFrame({\"sid\": np.array(result_df, dtype=\"int64\"), \"label\": pack})\r\n",
    "result_df.to_csv(RESULT_FILE, index=False)\r\n",
    "print(\"结果文件保存至：\", RESULT_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 代码审查\n",
    "如选手成绩靠前并收到官方邮件通知代码审查，请参考该[链接](https://aistudio.baidu.com/aistudio/projectdetail/743661)进行项目上传操作  \n",
    "快捷命令:`!zip -rP [此处添加审查邮件中的Key值] [邮件中的UID值].zip /home/aistudio/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: home/aistudio/ (stored 0%)\n",
      "  adding: home/aistudio/.conda/ (stored 0%)\n",
      "  adding: home/aistudio/.conda/environments.txt (deflated 14%)\n",
      "  adding: home/aistudio/.conda/pkgs/ (stored 0%)\n",
      "  adding: home/aistudio/.conda/pkgs/urls (stored 0%)\n",
      "  adding: home/aistudio/.conda/pkgs/urls.txt (stored 0%)\n",
      "  adding: home/aistudio/.viminfo (deflated 52%)\n",
      "  adding: home/aistudio/.virtual_documents/ (stored 0%)\n",
      "  adding: home/aistudio/.local/ (stored 0%)\n",
      "  adding: home/aistudio/.local/share/ (stored 0%)\n",
      "  adding: home/aistudio/.local/share/jupyter/ (stored 0%)\n",
      "  adding: home/aistudio/.local/share/jupyter/runtime/ (stored 0%)\n",
      "  adding: home/aistudio/.local/share/jupyter/runtime/nbserver-68.json (deflated 39%)\n",
      "  adding: home/aistudio/.local/share/jupyter/runtime/nbserver-68-open.html (deflated 44%)\n",
      "  adding: home/aistudio/.local/share/jupyter/runtime/kernel-7873bfc0-2726-45d3-8be1-ce0065940c88.json (deflated 37%)\n",
      "  adding: home/aistudio/.local/share/jupyter/nbsignatures.db (deflated 96%)\n",
      "  adding: home/aistudio/.local/share/jupyter/kernels/ (stored 0%)\n",
      "  adding: home/aistudio/.local/share/jupyter/kernels/py35-paddle1.2.0/ (stored 0%)\n",
      "  adding: home/aistudio/.local/share/jupyter/kernels/py35-paddle1.2.0/logo-32x32.png (stored 0%)\n",
      "  adding: home/aistudio/.local/share/jupyter/kernels/py35-paddle1.2.0/kernel.json (deflated 46%)\n",
      "  adding: home/aistudio/.local/share/jupyter/kernels/py35-paddle1.2.0/logo-64x64.png (stored 0%)\n",
      "  adding: home/aistudio/.local/share/jupyter/notebook_secret (deflated 22%)\n",
      "  adding: home/aistudio/.node.started (stored 0%)\n",
      "  adding: home/aistudio/.python_history (deflated 22%)\n",
      "  adding: home/aistudio/baseline_tools.py (deflated 57%)\n",
      "  adding: home/aistudio/.jupyter/ (stored 0%)\n",
      "  adding: home/aistudio/.jupyter/nbconfig/ (stored 0%)\n",
      "  adding: home/aistudio/.jupyter/nbconfig/notebook.json (deflated 41%)\n",
      "  adding: home/aistudio/.cache/ (stored 0%)\n",
      "  adding: home/aistudio/.cache/yarn/ (stored 0%)\n",
      "  adding: home/aistudio/.cache/yarn/v6/ (stored 0%)\n",
      "  adding: home/aistudio/.cache/yarn/v6/.tmp/ (stored 0%)\n",
      "  adding: home/aistudio/.bash_history (stored 0%)\n",
      "  adding: home/aistudio/output/ (stored 0%)\n",
      "  adding: home/aistudio/output/.ipynb_checkpoints/ (stored 0%)\n",
      "  adding: home/aistudio/.profile (deflated 41%)\n",
      "  adding: home/aistudio/__pycache__/ (stored 0%)\n",
      "  adding: home/aistudio/__pycache__/baseline_tools.cpython-37.pyc (deflated 44%)\n",
      "  adding: home/aistudio/.systemlogs/ (stored 0%)\n",
      "  adding: home/aistudio/.systemlogs/system.log (deflated 80%)\n",
      "  adding: home/aistudio/.bashrc (deflated 54%)\n",
      "  adding: home/aistudio/data/ (stored 0%)\n",
      "  adding: home/aistudio/.homedata.success (stored 0%)\n",
      "  adding: home/aistudio/.bash_logout (deflated 28%)\n",
      "  adding: home/aistudio/2035878.ipynb (deflated 72%)\n",
      "  adding: home/aistudio/work/ (stored 0%)\n",
      "  adding: home/aistudio/mini_train.csv (deflated 69%)\n",
      "  adding: home/aistudio/.config/ (stored 0%)\n",
      "  adding: home/aistudio/.config/flake8 (stored 0%)\n",
      "  adding: home/aistudio/.config/pycodestyle (stored 0%)\n",
      "  adding: home/aistudio/.pip/ (stored 0%)\n",
      "  adding: home/aistudio/.pip/pip.conf (deflated 16%)\n",
      "  adding: home/aistudio/emb_dicts/ (stored 0%)\n",
      "  adding: home/aistudio/emb_dicts/location.dict (deflated 65%)\n",
      "  adding: home/aistudio/emb_dicts/apptype.dict (deflated 62%)\n",
      "  adding: home/aistudio/emb_dicts/dev_height.dict (deflated 69%)\n",
      "  adding: home/aistudio/emb_dicts/carrier.dict (deflated 37%)\n",
      "  adding: home/aistudio/emb_dicts/lan.dict (deflated 42%)\n",
      "  adding: home/aistudio/emb_dicts/dev_ppi.dict (deflated 66%)\n",
      "  adding: home/aistudio/emb_dicts/os.dict (deflated 21%)\n",
      "  adding: home/aistudio/emb_dicts/ntt.dict (deflated 39%)\n",
      "  adding: home/aistudio/emb_dicts/dev_width.dict (deflated 69%)\n",
      "  adding: home/aistudio/emb_dicts/package.dict (deflated 66%)\n",
      "  adding: home/aistudio/emb_dicts/.ipynb_checkpoints/ (stored 0%)\n",
      "  adding: home/aistudio/emb_dicts/size.dict (deflated 34%)\n",
      "  adding: home/aistudio/emb_dicts/version.dict (deflated 45%)\n",
      "  adding: home/aistudio/emb_dicts/media_id.dict (deflated 67%)\n",
      "  adding: home/aistudio/emb_dicts/cus_type.dict (deflated 59%)\n",
      "  adding: home/aistudio/emb_dicts/osv.dict (deflated 63%)\n",
      "  adding: home/aistudio/.ipython/ (stored 0%)\n",
      "  adding: home/aistudio/.ipython/profile_default/ (stored 0%)\n",
      "  adding: home/aistudio/.ipython/profile_default/db/ (stored 0%)\n",
      "  adding: home/aistudio/.ipython/profile_default/startup/ (stored 0%)\n",
      "  adding: home/aistudio/.ipython/profile_default/startup/README (deflated 37%)\n",
      "  adding: home/aistudio/.ipython/profile_default/history.sqlite (deflated 98%)\n",
      "  adding: home/aistudio/.ipython/profile_default/security/ (stored 0%)\n",
      "  adding: home/aistudio/.ipython/profile_default/log/ (stored 0%)\n",
      "  adding: home/aistudio/.ipython/profile_default/pid/ (stored 0%)\n",
      "  adding: home/aistudio/.ipython/extensions/ (stored 0%)\n",
      "  adding: home/aistudio/.ipython/nbextensions/ (stored 0%)\n",
      "  adding: home/aistudio/.condarc (deflated 5%)\n",
      "  adding: home/aistudio/.ipynb_checkpoints/ (stored 0%)\n",
      "  adding: home/aistudio/.ipynb_checkpoints/2035878-checkpoint.ipynb (deflated 72%)\n",
      "  adding: home/aistudio/train.csv (deflated 88%)\n",
      "  adding: home/aistudio/.dataset.download/ (stored 0%)\n",
      "  adding: home/aistudio/.dataset.download/.dataset.done (stored 0%)\n",
      "  adding: home/aistudio/test.csv (deflated 87%)\n",
      "  adding: home/aistudio/.ssh/ (stored 0%)\n",
      "  adding: home/aistudio/.ssh/authorized_keys (stored 0%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r [黄贤博].zip /home/aistudio/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.0.0b0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
